<!DOCTYPE html><html lang="zh-cn" class="animation-prepared"> <head><title>开放词汇目标检测（OVD）</title><meta charset="utf-8"><link rel="canonical" href="https://blog.mslxl.com/posts/thesis/ovd/"><meta name="description" content="My story of discovery"><meta name="robots" content="index, follow"><meta property="og:title" content="开放词汇目标检测（OVD）"><meta property="og:type" content="article"><meta property="og:image" content="https://blog.mslxl.com/placeholder.png"><meta property="og:url" content="https://blog.mslxl.com"><link rel="icon" href="/favicon.jpg" type="image/jpg"><link rel="alternate" type="application/rss+xml" title="Integrate Life" href="https://blog.mslxl.com/atom.xml"><meta name="viewport" content="width=device-width"><meta name="generator" content="Astro v5.6.1"><meta name="twitter:image:src" content="https://blog.mslxl.com/placeholder.png"><meta name="twitter:image:alt" content="My story of discovery"><meta name="twitter:creator" content="@mslxl_bak"><meta name="twitter:site" content="@mslxl_bak"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:title" content="开放词汇目标检测（OVD）"><meta name="twitter:description" content="My story of discovery"><link rel="stylesheet" href="/_astro/_page_.CRPWmZtO.css">
<style>.tk-meta-input{background-color:#fff}
p[data-astro-cid-gcn2mc3v]{font-size:.75rem;font-weight:700}html.animation-prepared .transition-swup-header[data-astro-cid-e5lvscu2],html.animation-prepared .transition-swup-footer[data-astro-cid-e5lvscu2]{animation:fade-in-down 1s linear 1}@media (min-width: 1024px){html.animation-prepared .transition-swup-header[data-astro-cid-e5lvscu2],html.animation-prepared .transition-swup-footer[data-astro-cid-e5lvscu2]{animation-name:fade-in-left}}html.animation-prepared .transition-swup-main[data-astro-cid-e5lvscu2]{animation:fade-in-down 1s linear 1}@keyframes fade-in-down{0%{transform:translateY(-1rem);opacity:0}to{transform:translateY(0);opacity:1}}@keyframes fade-in-left{0%{opacity:0;transform:translate(1rem)}to{opacity:1;transform:translate(0)}}html.is-animating .transition-swup-main[data-astro-cid-e5lvscu2]{opacity:0;transform:translateY(-1rem)}html.is-leaving .transition-swup-main[data-astro-cid-e5lvscu2]{opacity:0;transform:translateY(1rem)}.transition-swup-main[data-astro-cid-e5lvscu2]{transition-duration:.5s;transition-timing-function:linear;transform:translateY(0);opacity:1}
.code-container{position:relative}.clipboard-copy{position:absolute;top:.5rem;right:.5rem;width:1.75rem;height:1.75rem;display:flex;justify-content:center;align-items:center;border-radius:.25rem}.clipboard-copy:hover{background-color:#30363d}
</style>
<link rel="stylesheet" href="/_astro/index.DuX3xYS3.css"><script type="module" src="/_astro/page.BTbYB4Kr.js"></script></head> <body>   <div class="h-100vh max-w-6xl min-w-390px mx-a grid-gap-x-28" p="7.5 lg:y-0 lg:x-20" lg="grid cols-[3fr_1fr] rows-[1fr_9rem]" data-astro-cid-e5lvscu2> <header class="transition-swup-header flex flex-col gap-2.5" m="7.5 lg:x-0 lg:t-20 lg:b-4" lg="row-1-2 col-2-3 justify-between items-start" data-astro-cid-e5lvscu2> <hgroup lg="write-vertical-right items-start b-l-2px b-l-primary-solid text-left" flex="~ col gap-2.5" class="cursor-pointer text-center duration-800 ease-in-out"> <a class="not-underline-hover duration-800 ease-in-out" lg:p="x-2.5 b-12 hover:t-3.75 hover:b-8.75" hover:lg=" bg-primary color-background" href="/"> <h3 class="text-5 font-extrabold font-header">Mslxl&#39;s Blog</h3> <h1 class="text-8 font-extrabold font-header">Integrate Life</h1> </a> </hgroup> <nav class="text-center font-bold" flex="~ col gap-4"> <ul lg="flex-col items-start text-4" class="text-3.5" flex="~ row gap-2 justify-center"> <li> <a class="" href="/"> 文章 </a> </li><li> <a class="" href="/archive"> 归档 </a> </li><li> <a class="" href="/categories"> 分类 </a> </li><li> <a class="" href="/tomodachi"> 友人 </a> </li><li> <a class="" href="/about"> 关于 </a> </li> </ul> <ul flex="~ row gap-1 justify-center"> <li> <a href="https://github.com/mslxl/" target="_blank" title="github" class="not-underline-hover inline-flex items-center"> <span class="i-mdi-github w-6 h-6">github</span> </a> </li><li> <a href="/atom.xml" target="_blank" title="rss" class="not-underline-hover inline-flex items-center"> <span class="i-mdi-rss w-6 h-6">rss</span> </a> </li><li> <a href="https://x.com/mslxl_bak/" target="_blank" title="twitter" class="not-underline-hover inline-flex items-center"> <span class="i-mdi-twitter w-6 h-6">twitter</span> </a> </li><li> <a href="https://mastodon.social/@mslxl" target="_blank" title="mastodon" class="not-underline-hover inline-flex items-center"> <span class="i-mdi-mastodon w-6 h-6">mastodon</span> </a> </li> </ul> </nav> </header> <main class="transition-swup-main link-fill overflow-y-scroll scrollbar-hide outline-none" lg="row-1-3 col-1-2 py-20 " data-astro-cid-e5lvscu2>   <article class="prose"> <header flex="~ col"> <h1 class="post-title"> <a class="not-prose" href="/posts/thesis/ovd/">开放词汇目标检测（OVD）</a> </h1> <div class="text-3.5"> <span>发布于</span> <time>2025-03-28</time> <a href="/categories/OVD">
# OVD </a><a href="/categories/mubiaojiance">
# 目标检测 </a><a href="/categories/AI">
# AI </a> </div> </header>  <h2 id="简介">简介</h2>
<p>在目标检测的发展历程中，存在以下几种方案的目标检测：</p>
<ul>
<li>Two-Stage: 两阶段的目标检测，向模型喂一张图。首先通过 Region Proposal 生成多个候选区域，接下来再进行筛选、检测获取目标。</li>
<li>One-Stage：一阶段的目标检测。端到端的形式，将图片输入到 backbone 中，直接输出检测框。最出色的即 YoLo 系列</li>
<li>Anchor-Based</li>
<li>Anchor-Free</li>
<li>Transformer</li>
</ul>
<p>不同于上面的方案，OVD 的目标是利用文本的语义信息和图片的语义信息结合起来，它关注图像的偏移，尤其是统计偏移。</p>
<p>图像的偏移有两种形式：</p>
<ul>
<li>统计偏移（纵坐标）</li>
<li>语义偏移（横坐标）</li>
</ul>
<p><img alt="图像偏移" width="1007" height="571" loading="lazy" decoding="async" src="/_astro/image-20250408144115045.C_7RcYtl_Z1IDRIU.webp" ></p>
<p>OVD 可以识别超过预定义的类别，借助弱监督信号（如图文对或 VLMs）。现有的大多数 OVD 方法都是基于 VLM（如 CLIP）。</p>
<p>不同于传统的分类任务，此处的对比学习将分配任务转换成了匹配任务。</p>
<p>传统的分配框架往往是先进行特征提取，随后加入 FC 进行分配，分类的结果是一个概率向量。</p>
<p>匹配任务采用对比学习中不仅提取了图片的特征，还将文本的特征同样提取了出来。随后将图像特征与文本特征进行匹配，概率最高的即为结果。</p>
<p>对比学习一个经典的例子即为 CLIP。它将 Text Embedding 与 Image Embedding 进行匹配，将符合结果的标记为正样本，其他为负样本。将两个 Encoder 生成的数据在表达相同结果时在同空间中最近。</p>
<p><img alt="CLIP 训练阶段" width="816" height="619" loading="lazy" decoding="async" src="/_astro/image-20250408145006867.iDjC_9nP_ZEd4VS.webp" ></p>
<p>CLIP 的推理相当简单，而且是 one-shot<sup><a href="#user-content-fn-1" id="user-content-fnref-1" data-footnote-ref="" aria-describedby="footnote-label">1</a></sup>的</p>
<p><img alt="推理" width="848" height="623" loading="lazy" decoding="async" src="/_astro/image-20250408145304981.DIKQjKdp_1kIMX8.webp" ></p>
<h2 id="框架">框架</h2>
<p>OVD 很类似 Two-Stage，它同样需要 RPN 进行 Region Proposal。</p>
<p><img alt="训练阶段" width="821" height="304" loading="lazy" decoding="async" src="/_astro/image-20250408150137086.BUNL1G1B_ZQ0bpy.webp" ></p>
<p>如上图，在训练阶段给出的数据包含图片、检测框和文本。通过图像编码器和检测头后对 Region 进行检测，然后再从文本中得到相应的类别的 embedding。再通过对比训练，计算分类 loss 和定位 loss。</p>
<p><img alt="推理阶段" width="805" height="346" loading="lazy" decoding="async" src="/_astro/image-20250408150610157.CY2T0Vk5_2p00sE.webp" ></p>
<h2 id="ovd-发展方向">OVD 发展方向</h2>
<ul>
<li>区域感知训练(region-aware training): 利用图文对，例如 group VIT</li>
<li>伪标签方法(pseudo-labeling)：利用图文对，在模型未完成训练时利用模型进行推理，推理的结果再重复训练</li>
<li>知识蒸馏(knowledge distillation): 大量使用 VLMs-IE</li>
<li>迁移学习(transfer learning)：大量利用 VLMs-IE</li>
</ul>
<h3 id="区域感知训练">区域感知训练</h3>
<p><img alt="GroupViT" width="613" height="463" loading="lazy" decoding="async" src="/_astro/image-20250408152146803.CdipdujM_Z1YxEny.webp" ></p>
<p>此方法并没有用到 VLM，而是利用文本对齐的方法。上图是个语义分割的例子：图像经过一系列的语义提取变为 embedding <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>z</mi><mi>I</mi></msup></mrow><annotation encoding="application/x-tex">z^I</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8413em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07847em;">I</span></span></span></span></span></span></span></span></span></span></span>，文本同理获得 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>T</mi></msub></mrow><annotation encoding="application/x-tex">z_T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。最终进行对比</p>
<p>区域感知其实就是将 CLIP 的思想运用到下游任务中，利用对比学习实现图像文本匹配。但是 CLIP 是整张图片与文本对齐，而区域感知训练则是图片中1个局部区域与N 个单词对齐。</p>
<h3 id="伪标签方法">伪标签方法</h3>
<p>该方法在 <em>Open Vocabulary Object Detection with Pseudo Bounding-Box Labels</em> 中提出，该文职提出能否利用现有的大规模的目标物体自动生成边界标签，利用这些生成的边界框提升 OVD 性能。</p>
<p><img alt="伪标签与人工方法对比" width="1198" height="320" loading="lazy" decoding="async" src="/_astro/image-20250408152659477.C3337DI7_Z1iWM9C.webp" ></p>
<p>上图中左侧为传统的学习方法，其训练成本很高。而右侧为伪标签方法。伪标签方法利用自己生成的标签训练自己。</p>
<h4 id="伪标签的生成">伪标签的生成</h4>
<p>该工作使用 VLM 生成伪标签。首先通过输入图文对分别提取特征，获得特征向量，然后利用图文交互的 cross-attention 机制获得多模态特征，计算图像区域和文本单词的注意力权重，再利用 Grad-CAM 对注意力权重进行可视化，得到目标的 Activation Map 区域，该局域即为 RPN 网络的目标 ROI。该ROI 构成该目标名称的伪边界框标签。</p>
<p><img alt="伪标签生成" width="990" height="327" loading="lazy" decoding="async" src="/_astro/image-20250408154837372.BcLF8fea_HwmDC.webp" ></p>
<h3 id="检测模型训练">检测模型训练</h3>
<p>通过伪标签数据即可训练 OVD 的检测。将图像数据根据伪标签获得 ROI，然后经过编码器得到特征向量，文本直接经过文本编码器得到向量。之后经过计算跨模态 embedding 的相似度，根据伪标签计算交叉熵。</p>
<h2 id="其他详细资料">其他详细资料</h2>
<p><a href="https://zhuanlan.zhihu.com/p/610639148">面向开放词汇的目标检测Open-Vocabulary Object Detection（OVD）的介绍</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/595169030">Open-Vocabulary Object Detection 工作调研</a></p>
<section data-footnotes="" class="footnotes"><h2 class="sr-only" id="footnote-label">Footnotes</h2>
<ol>
<li id="user-content-fn-1">
<p>指模型的泛化能力。Zero-Shot 指给出一张未出现在训练集中的数据，模型仍能给出结果。除此之外还有 One-Shot 和 Few-Shot。 One-Shot 指只需要一张图片进行 fine-tune（人脸识别就是一个常见场景），而 Few-Shot 只每个类别只有少量样本 <a href="#user-content-fnref-1" data-footnote-backref="" aria-label="Back to reference 1" class="data-footnote-backref">↩</a></p>
</li>
</ol>
</section>  </article> <footer class="mt-5">  <div class="flex items-center gap-2"> <a title="上一篇: Faster R-CNN" href="/posts/thesis/fasterrcnn"> <span class="inline-block vertical-mid i-mdi-chevron-double-left w-4 h-4"></span> 上一篇: Faster R-CNN </a> <a title="下一篇: 周报 0x02: 旧日幻梦" href="/posts/weekly/03feb25"> 下一篇: 周报 0x02: 旧日幻梦 <span class="inline-block vertical-mid i-mdi-chevron-double-right w-4 h-4"></span> </a> </div> </footer>  <div py-16><giscus-widget id="comments" repo="mslxl/mslxl.github.io" repoId="MDEwOlJlcG9zaXRvcnkxMjcwMTEwMjI=" category="Show and tell" categoryId="DIC_kwDOB5IIzs4CfW4f" mapping="title" strict="0" reactionsEnabled="1" emitMetadata="1" inputPosition="top" theme="light" lang="zh-CN" loading="lazy"></giscus-widget> <script type="module" src="/_astro/Giscus.astro_astro_type_script_index_0_lang.D05zVpD0.js"></script></div>  <div class="p-10" data-astro-cid-e5lvscu2> <div data-astro-cid-e5lvscu2> <img class="m-auto" src="https://moe-counter.glitch.me/get/@mslxl" alt="moe-counter" data-astro-cid-e5lvscu2> <p class="text-sm text-gray-500 text-center" data-astro-cid-e5lvscu2>统计自 2024 年 9 月</p> </div> </div> </main> <footer class="transition-swup-footer py-7.5" lg="row-2-3 col-2-3" data-astro-cid-e5lvscu2> <p data-astro-cid-gcn2mc3v>
© 2025 <a target="_blank" href="https://blog.mslxl.com" rel="noopener noreferrer" data-astro-cid-gcn2mc3v>Mslxl</a> </p> <p data-astro-cid-gcn2mc3v>
Theme
<a target="_blank" href="https://github.com/Moeyua/astro-theme-typography" rel="noopener noreferrer" data-astro-cid-gcn2mc3v>Typography</a>
by <a target="_blank" href="https://moeyua.com" rel="noopener noreferrer" data-astro-cid-gcn2mc3v>Moeyua</a> </p> <p data-astro-cid-gcn2mc3v>
Proudly published with
<a target="_blank" href="https://astro.build" rel="noopener noreferrer" data-astro-cid-gcn2mc3v>Astro</a> </p>  </footer> </div> <script type="module">document.addEventListener("animationend",e,!1);function e(){document.documentElement.classList.remove("animation-prepared"),document.removeEventListener("animationend",e,!1)}</script>   </body></html> <script type="module">function u(e){const t=document.createElement("pre");return t.style.width="1px",t.style.height="1px",t.style.position="fixed",t.style.top="5px",t.textContent=e,t}function a(e){if("clipboard"in navigator)return navigator.clipboard.writeText(e.textContent||"");const t=getSelection();if(t==null)return Promise.reject(new Error);t.removeAllRanges();const n=document.createRange();return n.selectNodeContents(e),t.addRange(n),document.execCommand("copy"),t.removeAllRanges(),Promise.resolve()}function i(e){if("clipboard"in navigator)return navigator.clipboard.writeText(e);const t=document.body;if(!t)return Promise.reject(new Error);const n=u(e);return t.appendChild(n),a(n),t.removeChild(n),Promise.resolve()}async function d(e){const t=e.getAttribute("for"),n=e.getAttribute("value");function o(){e.dispatchEvent(new CustomEvent("clipboard-copy",{bubbles:!0}))}if(e.getAttribute("aria-disabled")!=="true"){if(n)await i(n),o();else if(t){const r="getRootNode"in Element.prototype?e.getRootNode():e.ownerDocument;if(!(r instanceof Document||"ShadowRoot"in window&&r instanceof ShadowRoot))return;const c=r.getElementById(t);c&&(await p(c),o())}}}function p(e){return e instanceof HTMLInputElement||e instanceof HTMLTextAreaElement?i(e.value):e instanceof HTMLAnchorElement&&e.hasAttribute("href")?i(e.href):a(e)}function f(e){const t=e.currentTarget;t instanceof HTMLElement&&d(t)}function l(e){if(e.key===" "||e.key==="Enter"){const t=e.currentTarget;t instanceof HTMLElement&&(e.preventDefault(),d(t))}}function m(e){e.currentTarget.addEventListener("keydown",l)}function y(e){e.currentTarget.removeEventListener("keydown",l)}class h extends HTMLElement{static define(t="clipboard-copy",n=customElements){return n.define(t,this),this}constructor(){super(),this.addEventListener("click",f),this.addEventListener("focus",m),this.addEventListener("blur",y)}connectedCallback(){this.hasAttribute("tabindex")||this.setAttribute("tabindex","0"),this.hasAttribute("role")||this.setAttribute("role","button")}get value(){return this.getAttribute("value")||""}set value(t){this.setAttribute("value",t)}}const s=typeof globalThis<"u"?globalThis:window;try{s.ClipboardCopyElement=h.define()}catch(e){if(!(s.DOMException&&e instanceof DOMException&&e.name==="NotSupportedError")&&!(e instanceof ReferenceError))throw e}document.addEventListener("clipboard-copy",e=>{const n=e.target.querySelector(".icon");n.classList.replace("i-mdi-content-copy","i-mdi-check"),setTimeout(()=>{n.classList.replace("i-mdi-check","i-mdi-content-copy")},1500)});const b=Array.from(document.querySelectorAll("pre")),g="<div class='i-mdi-content-copy icon text-white'></div>";for(const e of b){const t=document.createElement("div");t.className="code-container";const n=document.createElement("clipboard-copy"),o=e.querySelector("code");n.value=o?.textContent??"",n.className="clipboard-copy",n.innerHTML=g,e.appendChild(n),e.parentNode?.insertBefore(t,e),t.appendChild(e)}</script> 